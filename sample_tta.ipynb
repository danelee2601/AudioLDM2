{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal code to perform text-to-audio (tta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dslee\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\dslee\\anaconda3\\envs\\audioldm\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from audioldm2 import text_to_audio, build_model, save_wave, get_time\n",
    "\n",
    "# # Set up environment\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AudioLDM-2: audioldm_48k\n",
      "Loading model on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dslee\\anaconda3\\envs\\audioldm\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\dslee\\anaconda3\\envs\\audioldm\\lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\dslee\\anaconda3\\envs\\audioldm\\lib\\site-packages\\torchaudio\\transforms\\_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Use extra condition on UNet channel using Film. Extra condition dimension is 512. \n",
      "DiffusionWrapper has 262.70 M params.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dslee\\anaconda3\\envs\\audioldm\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "d:\\projects\\AudioLDM2\\audioldm2\\pipeline.py:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(resume_from_checkpoint, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# Create save path\n",
    "model_name = \"audioldm_48k\"  # Choose the speech model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "save_path = \"./output\"  # Set save path\n",
    "# save_path = os.path.join(save_path, get_time())\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Load the model\n",
    "audioldm2 = build_model(model_name=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for TTS\n",
    "text = \"The sound of baby crying.\"\n",
    "transcription = \"\"\n",
    "duration = 10  # Duration in seconds; because all the training samples were 10s, the generated samples should be 10s long, otherwise it results in poor audio quality.\n",
    "guidance_scale = 3.5  # Guidance scale for text adherence\n",
    "ddim_steps = 200  # DDIM sampling steps\n",
    "n_candidate_gen_per_text = 3  # Number of candidate generations per text\n",
    "random_seed = np.random.randint(999999)\n",
    "sample_rate = 48000 if '48k' in model_name else 16000\n",
    "latent_t_per_second = 12.8 if '48k' in model_name else 25.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 200/200 [00:55<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: clap model calculate the audio embedding as condition\n",
      "Similarity between generated audio and text:\n",
      "0.49 0.42 0.47\n",
      "Choose the following indexes as the output: [0]\n",
      "Save audio to ./output\\TTS_The sound .wav\n",
      "Audio generated and saved at: ./output\n"
     ]
    }
   ],
   "source": [
    "# Generate TTS audio\n",
    "waveform = text_to_audio(\n",
    "    audioldm2,\n",
    "    text=text,\n",
    "    transcription=transcription,  # To guide the model based on the transcription\n",
    "    seed=random_seed,\n",
    "    duration=duration,\n",
    "    guidance_scale=guidance_scale,\n",
    "    ddim_steps=ddim_steps,\n",
    "    n_candidate_gen_per_text=n_candidate_gen_per_text,\n",
    "    batchsize=1,  # Single batch for simplicity\n",
    "    latent_t_per_second=latent_t_per_second\n",
    ")\n",
    "\n",
    "# Save the generated audio\n",
    "save_wave(waveform, save_path, name=f\"TTS_{text[:10]}\", samplerate=sample_rate)\n",
    "\n",
    "# Inform the user about the save location\n",
    "print(f\"Audio generated and saved at: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
